"""
ìŒì•… ê¸°ë°˜ ì§€ëŠ¥í˜• ì¶”ì¶œ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)

ê¸°ëŠ¥:
- ìŒì•…ê³¼ ìŒì„± ìë™ ì¸ì‹ ë° ì •í™•í•œ ì¶”ì¶œ
- ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- 23ë¶„ë¶€í„° ì „ì‚¬ (ì†ë„ í–¥ìƒ)
- ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì§€ì›
"""

import whisper
import os
import glob
from pydub import AudioSegment
import json
from pathlib import Path
import argparse

try:
    from inaSpeechSegmenter import Segmenter
    HAS_INA = True
except ImportError:
    HAS_INA = False


class SmartConversationExtractor:
    def __init__(self, model_size='tiny'):
        self.model_size = model_size
        self.model = None
        self.segmenter = None
        
    def load_models(self):
        """Whisper ë° inaSpeechSegmenter ë¡œë”©"""
        if self.model is None:
            print(f"ğŸ”„ Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (ëª¨ë¸: {self.model_size})")
            self.model = whisper.load_model(self.model_size)
            print("âœ… Whisper ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
        
        if HAS_INA and self.segmenter is None:
            print("ğŸ”„ inaSpeechSegmenter ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.segmenter = Segmenter()
            print("âœ… inaSpeechSegmenter ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")
    
    def extract_script_text(self, transcription, start_time, end_time):
        """
        ì§€ì •ëœ ì‹œê°„ ë²”ìœ„ì˜ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
        
        Args:
            transcription: Whisper ì „ì‚¬ ê²°ê³¼
            start_time: ì‹œì‘ ì‹œê°„ (ì´ˆ)
            end_time: ì¢…ë£Œ ì‹œê°„ (ì´ˆ)
            
        Returns:
            ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸
        """
        script_lines = []
        
        for segment in transcription['segments']:
            seg_start = segment['start']
            seg_end = segment['end']
            
            # ì¶”ì¶œ êµ¬ê°„ê³¼ ê²¹ì¹˜ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë§Œ
            if seg_start <= end_time and seg_end >= start_time:
                timestamp = f"[{seg_start/60:.2f}ë¶„ - {seg_end/60:.2f}ë¶„]"
                text = segment['text'].strip()
                script_lines.append(f"{timestamp} {text}")
        
        return "\n".join(script_lines)

    def extract_player_data(self, transcription, start_time, end_time):
        """
        ì›¹ í”Œë ˆì´ì–´ìš© ë°ì´í„° ì¶”ì¶œ (ìƒëŒ€ ì‹œê°„)
        """
        player_data = []
        
        for segment in transcription['segments']:
            seg_start = segment['start']
            seg_end = segment['end']
            
            if seg_start <= end_time and seg_end >= start_time:
                # ìƒëŒ€ ì‹œê°„ ê³„ì‚° (ì¶”ì¶œëœ ì˜¤ë””ì˜¤ì˜ ì‹œì‘ì´ 0ì´ˆ)
                rel_start = max(0, seg_start - start_time)
                rel_end = min(end_time - start_time, seg_end - start_time)
                
                player_data.append({
                    "start": round(rel_start, 2),
                    "end": round(rel_end, 2),
                    "text": segment['text'].strip()
                })
        
        return player_data
    
    def _is_korean(self, text):
        """í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        for char in text:
            if 'ê°€' <= char <= 'í£':
                return True
        return False
    
    def _is_english_segment(self, text, debug=False):
        """ì„¸ê·¸ë¨¼íŠ¸ê°€ ì£¼ë¡œ ì˜ì–´ì¸ì§€ íŒë‹¨ (ì˜ì–´ ì „ì‚¬ ê¸°ì¤€)"""
        text = text.strip()
        if not text:
            if debug: print(f"      âŒ ë¹ˆ í…ìŠ¤íŠ¸")
            return False
        
        # í•œê¸€ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í•œêµ­ì–´ (ì˜ì–´ ì „ì‚¬ì—ì„œë„ í•œê¸€ì´ ë‚¨ì„ ìˆ˜ ìˆìŒ)
        if any('ê°€' <= c <= 'í£' for c in text):
            if debug: print(f"      âŒ í•œê¸€ í¬í•¨")
            return False
        
        # ì˜ì–´ ì „ì‚¬ ëª¨ë“œì—ì„œ:
        # 1. ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ë¬´ì‹œ (hallucinationì´ë‚˜ ë…¸ì´ì¦ˆ)
        if len(text) < 12:  # ì™„í™”: 20 â†’ 12 (ì§§ì€ ì‘ë‹µ í—ˆìš©)
            if debug: print(f"      âŒ ë„ˆë¬´ ì§§ìŒ ({len(text)}ì < 12ì)")
            return False
        
        # 2. í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ì˜ëª» ì „ì‚¬í•œ íŒ¨í„´ ê°ì§€
        korean_transliteration_patterns = [
            'ì…ì˜ì‘', 'íƒ€ì„', 'íŒ¨í„´', 'ë§Œë‚˜ë³¼ê¹Œìš”', 'ì—°ê¸°', 'ì—°ìŠµ',
            'ì½ì–´ë³¼ê²Œìš”', 'ì „ì²´ëŒ€í™”', 'ë“£ê² ìŠµë‹ˆë‹¤', 'ì£¼ì„¸ìš”',
            'ê·¸ë ‡ì£ ', 'ì—¬ëŸ¬ë¶„', 'ì´ê±°', 'í™œìš©', 'ê°ˆê²Œìš”',
            'ì¡¸ì—…í–ˆì–´', 'ëºì–´', 'íŒŒìš´ë“œ', 'ê°œì›”', 'kg'
        ]
        
        text_lower = text.lower()
        for pattern in korean_transliteration_patterns:
            if pattern.lower() in text_lower:
                if debug: print(f"      âŒ í•œêµ­ì–´ íŒ¨í„´ '{pattern}' ê°ì§€")
                return False
        
        # 3. ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ ì˜ì‹¬  
        words = text.split()
        if len(words) < 2:  # ì™„í™”: 3 â†’ 2 ("Please." ë“± ì§§ì€ ì‘ë‹µ í—ˆìš©)
            if debug: print(f"      âŒ ë‹¨ì–´ ìˆ˜ ë¶€ì¡± ({len(words)}ê°œ < 2ê°œ)")
            return False
        
        # í•œê¸€ê³¼ í•œêµ­ì–´ íŒ¨í„´ ì²´í¬ë§Œìœ¼ë¡œë„ ì¶©ë¶„ - ì˜ì–´ ë‹¨ì–´ ê²€ì¦ì€ ë„ˆë¬´ ì œí•œì 
        if debug: print(f"      âœ… í†µê³¼!")
        return True
    
    def find_anchor_and_extract_smart(self, audio_path,
                                      search_start_time=1380,
                                      anchor_phrases=["ì „ì²´ëŒ€í™” ì£¼ì„¸ìš”", "ì „ì²´ëŒ€í™”", "ì „ì²´ ëŒ€í™”", "ì „ì²´ë˜ì–´", "ì „ì²´ ë˜ì–´"]):
        """
        ìŒì•… ê¸°ë°˜ ì§€ëŠ¥í˜• ì¶”ì¶œ + Whisper ì „ì‚¬ë¡œ ì˜ì–´ êµ¬ê°„ë§Œ í•„í„°ë§
        
        Returns:
            (ì„±ê³µ ì—¬ë¶€, ì•µì»¤ ì‹œê°„, ì¶”ì¶œ íŒŒì¼ ê²½ë¡œ)
        """
        print(f"{'='*80}")
        print(f"ğŸµ íŒŒì¼: {os.path.basename(audio_path)}")
        print(f"{'='*80}\n")
        
        # ì˜¤ë””ì˜¤ ë¡œë”©
        audio_full = AudioSegment.from_mp3(audio_path)
        start_ms = search_start_time * 1000
        audio_segment = audio_full[start_ms:]
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        temp_path = "temp_segment.mp3"
        audio_segment.export(temp_path, format="mp3")
        
        # 1ë‹¨ê³„: í•œêµ­ì–´ ì „ì‚¬ë¡œ ì•µì»¤ ì°¾ê¸°
        print(f"ğŸ”„ 1ë‹¨ê³„: í•œêµ­ì–´ ì „ì‚¬ë¡œ ì•µì»¤ ì°¾ê¸°...")
        result_ko = self.model.transcribe(
            temp_path,
            language='ko',
            word_timestamps=False,
            verbose=False
        )
        
        # ì‹œê°„ ì˜¤í”„ì…‹ ë³´ì • (23ë¶„ ì¶”ê°€)
        for segment in result_ko['segments']:
            segment['start'] += search_start_time
            segment['end'] += search_start_time
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(temp_path)
        
        # ì „ì‚¬ ê²°ê³¼ ì €ì¥ (í•œêµ­ì–´)
        base_name = Path(audio_path).stem
        transcription_path = f"transcription_{base_name}.json"
        with open(transcription_path, 'w', encoding='utf-8') as f:
            json.dump(result_ko, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ í•œêµ­ì–´ ì „ì‚¬ ê²°ê³¼ ì €ì¥: {transcription_path}\n")
        
        # 2. ì•µì»¤ ê²€ìƒ‰ (í•œêµ­ì–´ ì „ì‚¬ ì‚¬ìš©)
        print(f"ğŸ” ì•µì»¤ ë¬¸êµ¬ ê²€ìƒ‰ ì¤‘...")
        anchor_end_time = None
        segments_ko = result_ko['segments']
        
        # ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ ê²€ìƒ‰
        for segment in segments_ko:
            text = segment['text'].strip()
            
            for anchor in anchor_phrases:
                if anchor in text:
                    anchor_end_time = segment['end']
                    print(f"âœ… ì•µì»¤ ë°œê²¬!")
                    print(f"   í…ìŠ¤íŠ¸: '{text}'")
                    print(f"   ì‹œê°„: {anchor_end_time:.2f}ì´ˆ ({anchor_end_time/60:.2f}ë¶„)\n")
                    break
            
            if anchor_end_time:
                break
        
        # ë³‘í•© ê²€ìƒ‰
        if anchor_end_time is None:
            print(f"ğŸ” ì—°ì† ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•© ê²€ìƒ‰ ì¤‘...")
            for i, segment in enumerate(segments_ko):
                if i < len(segments_ko) - 2:
                    combined_text = (
                        segment['text'] + 
                        segments_ko[i+1]['text'] + 
                        segments_ko[i+2]['text']
                    ).strip()
                    
                    for anchor in anchor_phrases:
                        if anchor in combined_text:
                            anchor_end_time = segments_ko[i+2]['end']
                            print(f"âœ… ì•µì»¤ ë°œê²¬ (ë³‘í•©)!")
                            print(f"   í…ìŠ¤íŠ¸: '{combined_text}'")
                            print(f"   ì‹œê°„: {anchor_end_time:.2f}ì´ˆ ({anchor_end_time/60:.2f}ë¶„)\n")
                            break
                    
                    if anchor_end_time:
                        break
        
        if anchor_end_time is None:
            print(f"âŒ ì•µì»¤ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤\n")
            return False, None, None
        
        
        # 2ë‹¨ê³„: ìŒì•… êµ¬ê°„ ê°ì§€ë¡œ ëŒ€í™” ì¶”ì¶œ
        print(f"ğŸ”„ 2ë‹¨ê³„: ìŒì•… êµ¬ê°„ ê°ì§€ ì¤‘...")
        
        # inaSpeechSegmenterë¡œ ìŒì•…/ëŒ€í™” êµ¬ê°„ ë¶„ì„
        if not HAS_INA or self.segmenter is None:
            print("âŒ inaSpeechSegmenterê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print("ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
            print("  pip install inaSpeechSegmenter tensorflow\n")
            return False, None, None
        
        print("ğŸ¼ ìŒì•… ë° ìŒì„± ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ ì¤‘...")
        ina_segments = self.segmenter(audio_path)
        
        # ì•µì»¤ ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ë§Œ í•„í„°ë§
        target_segments = [(label, start, end) for label, start, end in ina_segments if start >= anchor_end_time]
        
        if not target_segments:
            print("âš ï¸  ì•µì»¤ ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n")
            return False, None, None
        
        print(f"\nğŸ“Š ì•µì»¤ ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ (ì²˜ìŒ 20ê°œ):")
        for i, (label, start, end) in enumerate(target_segments[:20]):
            duration = end - start
            print(f"  {i+1:2d}. {label:12s} {start:7.2f}ì´ˆ ~ {end:7.2f}ì´ˆ (ê¸¸ì´: {duration:5.2f}ì´ˆ)")
        if len(target_segments) > 20:
            print(f"  ... ì™¸ {len(target_segments)-20}ê°œ ì„¸ê·¸ë¨¼íŠ¸")
        
        # 3. ìŒì•… ì‹œì‘ì  ì°¾ê¸° (ì•µì»¤ ì´í›„ ì²« ìŒì•…)
        extract_start = None
        for label, start, end in target_segments:
            if label == 'music':
                extract_start = start
                print(f"\n  ğŸµ ìŒì•… ì‹œì‘: {start:.2f}ì´ˆ ({start/60:.2f}ë¶„)")
                break
        
        if extract_start is None:
            print("\nâš ï¸  ìŒì•…ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì•µì»¤ ì§í›„ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.")
            extract_start = anchor_end_time
        
        
        
        # 4. ìŒì•… ì¢…ë£Œì  ì°¾ê¸° (ìŒì•… ëë‚˜ê³  í•œêµ­ì–´ ê³„ì† ë‚˜ì˜¤ë©´ ì¢…ë£Œ)
        # í•œêµ­ì–´ ì „ì‚¬ ë°ì´í„°ì—ì„œ ì•µì»¤ ì´í›„ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ì¶œ
        korean_segments_after_anchor = [
            (seg['start'], seg['end'], seg['text']) 
            for seg in result_ko['segments'] 
            if seg['start'] > anchor_end_time + 5  # ì•µì»¤ 5ì´ˆ í›„ë¶€í„°
        ]
        
        print(f"\n  ğŸ“‹ ì•µì»¤ ì´í›„ í•œêµ­ì–´ ì„¸ê·¸ë¨¼íŠ¸ (ì²˜ìŒ 10ê°œ):")
        for i, (start, end, text) in enumerate(korean_segments_after_anchor[:10]):
            print(f"    {i+1}. [{start:.1f}s] {text[:50]}")
        
        extract_end = extract_start
        segment_count = 0
        
        # í•œêµ­ì–´ ì„¸ê·¸ë¨¼íŠ¸ì˜ ì—°ì†ì„± ë¶„ì„: ì§„ì§œ í•œêµ­ì–´ëŠ” ì´˜ì´˜í•˜ê²Œ, ì˜ëª»ëœ ì „ì‚¬ëŠ” ë“œë¬¸ë“œë¬¸
        def find_teacher_explanation_start():
            """ì—°ì†ëœ í•œêµ­ì–´ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì°¾ì•„ ì§„ì§œ ì„ ìƒë‹˜ ì„¤ëª… ì‹œì‘ì  ë°˜í™˜"""
            for i in range(len(korean_segments_after_anchor) - 2):
                seg1_start, seg1_end, seg1_text = korean_segments_after_anchor[i]
                seg2_start, seg2_end, seg2_text = korean_segments_after_anchor[i + 1]
                seg3_start, seg3_end, seg3_text = korean_segments_after_anchor[i + 2]
                
                gap1 = seg2_start - seg1_start
                gap2 = seg3_start - seg2_start
                
                # 3ê°œ ì—°ì† ì„¸ê·¸ë¨¼íŠ¸ê°€ ê°ê° 5ì´ˆ ì´ë‚´ ê°„ê²© â†’ ì§„ì§œ í•œêµ­ì–´ ì„¤ëª…
                if gap1 <= 5.0 and gap2 <= 5.0:
                    print(f"\n  ğŸ“ ì§„ì§œ í•œêµ­ì–´ ì„¤ëª… ê°ì§€:")
                    print(f"    [{seg1_start:.1f}s] {seg1_text[:30]}")
                    print(f"    [{seg2_start:.1f}s] {seg2_text[:30]} (gap: {gap1:.1f}s)")
                    print(f"    [{seg3_start:.1f}s] {seg3_text[:30]} (gap: {gap2:.1f}s)")
                    return seg1_start
            
            return None
        
        teacher_start = find_teacher_explanation_start()
        
        print(f"\n  ğŸ” ì„¸ê·¸ë¨¼íŠ¸ ì²˜ë¦¬ ì¤‘:")
        for label, start, end in target_segments:
            if start >= extract_start:
                segment_count += 1
                duration = end - start
                
                # ì„ ìƒë‹˜ ì„¤ëª… ì‹œì‘ì  ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
                if teacher_start and end > teacher_start:
                    print(f"    {segment_count}. {label.upper():7s} [{start:.1f}s-{end:.1f}s] ({duration:.1f}s)")
                    print(f"\n  â¹ï¸  ì„ ìƒë‹˜ ì„¤ëª… ì‹œì‘ ({teacher_start:.1f}s) ì „ì— ì¶”ì¶œ ì¢…ë£Œ")
                    print(f"  â¹ï¸  {extract_end:.2f}ì´ˆì—ì„œ ì¶”ì¶œ ì¢…ë£Œ")
                    break
                
                if label == 'music':
                    extract_end = end
                    print(f"    {segment_count}. MUSIC   [{start:.1f}s-{end:.1f}s] ({duration:.1f}s) âœ… í¬í•¨ (extract_end={extract_end:.1f})")
                    
                elif label in ['male', 'female']:
                    extract_end = end
                    print(f"    {segment_count}. {label.upper():7s} [{start:.1f}s-{end:.1f}s] ({duration:.1f}s) âœ… í¬í•¨ (extract_end={extract_end:.1f})")
                    
                elif label == 'noEnergy':
                    print(f"    {segment_count}. SILENCE [{start:.1f}s-{end:.1f}s] ({duration:.1f}s) â­ï¸ ê±´ë„ˆëœ€")
                    
                else:
                    print(f"    {segment_count}. {label.upper():7s} [{start:.1f}s-{end:.1f}s] ({duration:.1f}s) â­ï¸ ê±´ë„ˆëœ€")
        
        print(f"\n  ì´ ì²˜ë¦¬í•œ ì„¸ê·¸ë¨¼íŠ¸: {segment_count}ê°œ")
        
        
        # 5. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        duration = extract_end - extract_start
        print(f"\nâœ‚ï¸  êµ¬ê°„ ì¶”ì¶œ:")
        print(f"   ì‹œì‘: {extract_start:.2f}ì´ˆ ({extract_start/60:.2f}ë¶„)")
        print(f"   ì¢…ë£Œ: {extract_end:.2f}ì´ˆ ({extract_end/60:.2f}ë¶„)")
        print(f"   ê¸¸ì´: {duration:.2f}ì´ˆ\n")
        
        start_ms = int(extract_start * 1000)
        end_ms = int(extract_end * 1000)
        
        extracted = audio_full[start_ms:end_ms]
        
        # MP3 ì €ì¥
        output_path = f"extracted_{base_name}.mp3"
        print(f"ğŸ’¾ ì €ì¥ ì¤‘: {output_path}")
        extracted.export(
            output_path,
            format='mp3',
            bitrate='320k',
            parameters=["-q:a", "0"]
        )
        
        actual_duration = len(extracted) / 1000
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {actual_duration:.1f}ì´ˆ\n")
        
        # 6. ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•œêµ­ì–´ ì „ì‚¬ ì‚¬ìš©)
        script_text = self.extract_script_text(result_ko, extract_start, extract_end)
        script_path = f"script_{base_name}.txt"
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(f"{'='*80}\n")
            f.write(f"ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸: {os.path.basename(audio_path)}\n")
            f.write(f"{'='*80}\n")
            f.write(f"êµ¬ê°„: {extract_start:.2f}ì´ˆ ~ {extract_end:.2f}ì´ˆ ({duration:.2f}ì´ˆ)\n")
            f.write(f"{'='*80}\n\n")
            f.write(script_text)
        
        print(f"ğŸ“ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì €ì¥: {script_path}\n")

        print(f"ğŸ“ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì €ì¥: {script_path}\n")

        # 7. ì›ì–´ë¯¼ ëŒ€í™” ì „ìš© ì˜ì–´ ì „ì‚¬ (ì›¹ í”Œë ˆì´ì–´ìš©)
        print(f"ğŸ”„ 7ë‹¨ê³„: ì›ì–´ë¯¼ ëŒ€í™” ê³ í’ˆì§ˆ ì˜ì–´ ì „ì‚¬ ì¤‘...")
        result_en = self.model.transcribe(
            output_path,
            language='en',
            word_timestamps=False,
            verbose=False
        )
        
        # 8. ì›¹ í”Œë ˆì´ì–´ìš© JSON ë°ì´í„° ì €ì¥ (ì˜ì–´ ì „ì‚¬ ê²°ê³¼ ì‚¬ìš©, 0ì´ˆ ê¸°ì¤€)
        player_data = self.extract_player_data(result_en, 0, actual_duration)
        player_json_path = f"player_{base_name}.json"
        with open(player_json_path, 'w', encoding='utf-8') as f:
            json.dump({
                "audio": output_path,
                "script": player_data
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“± ì›¹ í”Œë ˆì´ì–´ ë°ì´í„° ì €ì¥(ì˜ì–´): {player_json_path}\n")
        
        return True, anchor_end_time, output_path
    
    def _extract_fixed(self, audio_path, anchor_end_time, base_name, transcription):
        """ê³ ì • ì‹œê°„ ì¶”ì¶œ (fallback)"""
        start_offset = 46
        duration = 50
        
        actual_start = anchor_end_time + start_offset
        actual_end = actual_start + duration
        
        print(f"\nâœ‚ï¸  ê³ ì • êµ¬ê°„ ì¶”ì¶œ:")
        print(f"   ì‹œì‘: {actual_start:.2f}ì´ˆ")
        print(f"   ê¸¸ì´: {duration}ì´ˆ\n")
        
        audio = AudioSegment.from_mp3(audio_path)
        start_ms = int(actual_start * 1000)
        end_ms = min(start_ms + (duration * 1000), len(audio))
        
        extracted = audio[start_ms:end_ms]
        
        output_path = f"extracted_{base_name}.mp3"
        print(f"ğŸ’¾ ì €ì¥ ì¤‘: {output_path}")
        extracted.export(
            output_path,
            format='mp3',
            bitrate='320k',
            parameters=["-q:a", "0"]
        )
        
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(extracted)/1000:.1f}ì´ˆ\n")
        
        # ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì¶”ì¶œ
        script_text = self.extract_script_text(transcription, actual_start, actual_end)
        script_path = f"script_{base_name}.txt"
        
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(f"{'='*80}\n")
            f.write(f"ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸: {os.path.basename(audio_path)}\n")
            f.write(f"{'='*80}\n")
            f.write(f"êµ¬ê°„: {actual_start:.2f}ì´ˆ ~ {actual_end:.2f}ì´ˆ ({duration:.2f}ì´ˆ)\n")
            f.write(f"{'='*80}\n\n")
            f.write(script_text)
        
        print(f"ğŸ“ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ì €ì¥: {script_path}\n")

        # ì›ì–´ë¯¼ ëŒ€í™” ì „ìš© ì˜ì–´ ì „ì‚¬ (ì›¹ í”Œë ˆì´ì–´ìš©)
        print(f"ğŸ”„ ì›ì–´ë¯¼ ëŒ€í™” ê³ í’ˆì§ˆ ì˜ì–´ ì „ì‚¬ ì¤‘ (ê³ ì • êµ¬ê°„)...")
        result_en = self.model.transcribe(
            output_path,
            language='en',
            word_timestamps=False,
            verbose=False
        )

        # ì›¹ í”Œë ˆì´ì–´ìš© JSON ë°ì´í„° ì €ì¥ (ì˜ì–´ ì „ì‚¬ ê²°ê³¼ ì‚¬ìš©, 0ì´ˆ ê¸°ì¤€)
        player_data = self.extract_player_data(result_en, 0, duration)
        player_json_path = f"player_{base_name}.json"
        with open(player_json_path, 'w', encoding='utf-8') as f:
            json.dump({
                "audio": output_path,
                "script": player_data
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“± ì›¹ í”Œë ˆì´ì–´ ë°ì´í„° ì €ì¥ (ì˜ì–´/ê³ ì • êµ¬ê°„): {player_json_path}\n")
        
        return True, anchor_end_time, output_path
    
    def process_folder(self, folder_path='.',
                      pattern='*.mp3',
                      exclude_patterns=['extracted_', 'transcription_', 'ì™•ì´ˆë³´ì˜ì–´_']):
        """í´ë” ë‚´ MP3 íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        self.load_models()
        
        search_path = os.path.join(folder_path, pattern)
        all_files = glob.glob(search_path)
        
        mp3_files = []
        for f in all_files:
            basename = os.path.basename(f)
            should_exclude = False
            for exclude_pattern in exclude_patterns:
                if basename.startswith(exclude_pattern):
                    should_exclude = True
                    break
            if not should_exclude:
                mp3_files.append(f)
        
        if not mp3_files:
            print(f"âš ï¸  ì²˜ë¦¬í•  MP3 íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            return
        
        print(f"\n{'='*80}")
        print(f"ğŸ“ í´ë”: {os.path.abspath(folder_path)}")
        print(f"ğŸµ ë°œê²¬ëœ íŒŒì¼: {len(mp3_files)}ê°œ")
        print(f"{'='*80}\n")
        
        results = []
        
        for i, file_path in enumerate(mp3_files, 1):
            print(f"[{i}/{len(mp3_files)}] ì²˜ë¦¬ ì¤‘...\n")
            
            success, anchor_time, output_path = self.find_anchor_and_extract_smart(file_path)
            
            results.append({
                'file': os.path.basename(file_path),
                'success': success,
                'anchor_time': anchor_time,
                'output': os.path.basename(output_path) if output_path else None
            })
            
            print()
        
        # ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*80}")
        print(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print(f"{'='*80}\n")
        
        success_count = sum(1 for r in results if r['success'])
        print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}ê°œ\n")
        
        for r in results:
            status = "âœ…" if r['success'] else "âŒ"
            print(f"{status} {r['file']}")
            if r['success']:
                print(f"   â†’ {r['output']} (ì•µì»¤: {r['anchor_time']:.1f}ì´ˆ)")
        
        print(f"\n{'='*80}")
        print(f"ğŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"{'='*80}\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='EBS ì˜ì–´ ê°•ì˜ ëŒ€í™” êµ¬ê°„ ìë™ ì¶”ì¶œ')
    parser.add_argument('--file', '-f', type=str, help='ì²˜ë¦¬í•  íŠ¹ì • MP3 íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--folder', type=str, default='.', help='ì²˜ë¦¬í•  í´ë” (ê¸°ë³¸: í˜„ì¬ í´ë”)')
    parser.add_argument('--model', type=str, default='tiny', choices=['tiny', 'base', 'small', 'medium', 'large'],
                       help='Whisper ëª¨ë¸ í¬ê¸° (ê¸°ë³¸: tiny)')
    
    args = parser.parse_args()
    
    if not HAS_INA:
        print("="*80)
        print("âš ï¸  inaSpeechSegmenterê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        print("="*80)
        print("\në‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("  pip install inaSpeechSegmenter tensorflow\n")
        print("ì„¤ì¹˜ ì—†ì´ ê³„ì†í•˜ë ¤ë©´ fast_extract.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
        print("="*80)
        return
    
    extractor = SmartConversationExtractor(model_size=args.model)
    
    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
    if args.file:
        if not os.path.exists(args.file):
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.file}")
            return
        
        extractor.load_models()
        success, anchor_time, output_path = extractor.find_anchor_and_extract_smart(args.file)
        
        if success:
            print(f"\nâœ… ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"   ì¶œë ¥ íŒŒì¼: {output_path}")
        else:
            print("\nâŒ ì²˜ë¦¬ ì‹¤íŒ¨")
    
    # í´ë” ì „ì²´ ì²˜ë¦¬
    else:
        extractor.process_folder(
            folder_path=args.folder,
            pattern='*.mp3',
            exclude_patterns=['extracted_', 'transcription_', 'ì™•ì´ˆë³´ì˜ì–´_', 'script_']
        )


if __name__ == "__main__":
    main()
